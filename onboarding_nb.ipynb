{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExplainableRL: Onboarding recipe\n",
    "In this notebook, a template sequence of steps to use the ExplainableRL library is described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: give a bit of context and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import *\n",
    "\n",
    "# Import functions\n",
    "from src.foundation.engine import Engine\n",
    "from src.data_handler.data_handler import DataHandler\n",
    "from src.explainability.pdp import PDP\n",
    "from src.explainability.shap_values import ShapValues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain each parameter and when to use what\n",
    "hyperparam_dict_ds_data_suggest = {\n",
    "    \"dimensions\": {'states': {'lead_time': 10,\n",
    "                                'length_of_stay': 10,\n",
    "                                'competitor_price_difference_bin': 4,\n",
    "                                'demand_bin': 4,\n",
    "                                'price': 4},\n",
    "                    'actions': {'price': 10},\n",
    "                    'rewards': ['reward']\n",
    "                    },\n",
    "\n",
    "    \"dataset\": {'data_path': 'data/ds-data/my_example_data.parquet',\n",
    "                'col_delimiter': '|',\n",
    "                'n_samples': 1000,\n",
    "                'normalisation': True},\n",
    "\n",
    "    \"training\": {'env_type': 'strategic_pricing_suggest',\n",
    "                    'num_episodes': 500,\n",
    "                    'num_steps': 1,\n",
    "                    'train_test_split': 0.2,\n",
    "                    'evaluate': False,\n",
    "                    'num_eval_steps': 1},\n",
    "\n",
    "    \"agent\": {'agent_type': 'q_learner',\n",
    "                \"gamma\": 0.3,\n",
    "                \"epsilon\": 0.4,\n",
    "                \"epsilon_decay\": 0.1,\n",
    "                \"epsilon_minimum\": 0.1,\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"learning_rate_decay\": 0.1,\n",
    "                \"learning_rate_minimum\": 0.1,\n",
    "                \"lambda\": 0.2,\n",
    "                \"use_uncertainty\": False,\n",
    "                \"q_importance\": 0.7,\n",
    "                },\n",
    "\n",
    "    \"explainability\": {'shap_num_samples': 1},\n",
    "\n",
    "    \"program_flow\": {\"verbose\": False}\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: overview of what to initialise (DataHandler & Engine with Agent and Env inside)\n",
    "# TODO: If needed by evaluation/explainability/performance add a new dictionary and explain it\n",
    "hyperparam_dict = hyperparam_dict_ds_data_suggest\n",
    "verbose = hyperparam_dict['program_flow']['verbose']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a. Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to define the functions to load data from the path, and split it into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain data strucuture needed for both Env & how DataHanlder is built\n",
    "\n",
    "# TODO: Load and split the data in train and test and save the test set\n",
    "def load_data(data_path, delimiter=','):\n",
    "    \"\"\"Load data from file.\n",
    "\n",
    "    Args:\n",
    "        delimiter (str): Which separates columns.\n",
    "    \"\"\"\n",
    "    file_type = data_path.split('.')[-1]\n",
    "    if file_type == 'csv':\n",
    "        dataset = pd.read_csv(data_path, sep=delimiter)\n",
    "    elif file_type == 'xlsx':\n",
    "        dataset = pd.read_excel(data_path)\n",
    "    elif file_type == 'parquet':\n",
    "        dataset = pd.read_parquet(data_path)\n",
    "    else:\n",
    "        raise ValueError(\"File type not supported\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_train_test(dataset, train_test_split=0.2):\n",
    "    \"\"\"Split dataset into train and test.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Dataset.\n",
    "        train_test_split (float): Proportion of test data.\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset (pd.DataFrame): Train dataset.\n",
    "        test_dataset (pd.DataFrame): Test dataset.\n",
    "    \"\"\"\n",
    "    msk = np.random.rand(len(dataset)) < train_test_split\n",
    "    return dataset[msk], dataset[~msk]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and split it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(hyperparam_dict['dataset']['data_path'])\n",
    "train_dataset, test_dataset = split_train_test(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Datahandler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataHandler.__init__() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/usuari/Desktop/Imperial/mscai/Term 2/software-engineering-group/explainable-RL/onboarding_nb.ipynb Cell 15\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/usuari/Desktop/Imperial/mscai/Term%202/software-engineering-group/explainable-RL/onboarding_nb.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm/\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/usuari/Desktop/Imperial/mscai/Term%202/software-engineering-group/explainable-RL/onboarding_nb.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtimestamp\u001b[39m}\u001b[39;00m\u001b[39m: Load data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/usuari/Desktop/Imperial/mscai/Term%202/software-engineering-group/explainable-RL/onboarding_nb.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dh \u001b[39m=\u001b[39m DataHandler(hyperparam_dict\u001b[39m=\u001b[39;49mhyperparam_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataHandler.__init__() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Load data\")\n",
    "dh = DataHandler(dataset=train_dataset, hyperparam_dict=hyperparam_dict, test_dataset=test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b. Engine Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Load data\")\n",
    "dh = DataHandler(hyperparam_dict=hyperparam_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c. Environment and Agent Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Initialise Engine\")\n",
    "engine = Engine(dh=dh, hyperparam_dict=hyperparam_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Create the world\")\n",
    "engine.create_world()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Agent Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    n_samples = hyperparam_dict['n_samples']\n",
    "    n_episodes = hyperparam_dict['n_episodes']\n",
    "    print(f\"{timestamp}: Train the agent on {n_samples} samples and {n_episodes} episodes\")\n",
    "engine.train_agent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b. Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# TODO: Add saving and uploading process in pickle\n",
    "# TODO: Take path and file name from dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# TODO: Adapt to new main - Change and adapt\n",
    "\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Evaluate agent\")\n",
    "\n",
    "states, actions, b_actions, rewards_hist, actions_agent, b_actions_agent, rewards_agent = \\\n",
    "    engine.evaluate_agent()\n",
    "\n",
    "# Sum obtained reward optimal vs historical policy\n",
    "print(f\"Return based on historical data: {np.sum(rewards_hist)}\")\n",
    "print(f\"Return based on agent policy: {np.sum(rewards_agent)}\")\n",
    "\n",
    "# Plot perfomance\n",
    "plt.scatter(actions, actions_agent)\n",
    "plt.savefig('policy.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation on explainability framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a. PDP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Show PDPs plots\")\n",
    "pdp = PDP(engine=engine)\n",
    "pdp.build_data_for_plots(engine.agent.Q, engine.agent.Q_num_samples)\n",
    "type_features = hyperparam_dict['feature_types']\n",
    "fig_name = \"PDP plots - All states\"\n",
    "pdp.plot_pdp(states_names=state_labels, \n",
    "             fig_name=fig_name,\n",
    "             type_features=type_features, \n",
    "             savefig=True, \n",
    "             all_states=True)\n",
    "# TODO: Change PDP parameters based on new parameters dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b. Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Show SHAP values plots\")\n",
    "shap_values = ShapValues(sample=[8, 1, 1, 1],\n",
    "                         engine=engine,\n",
    "                         number_of_samples=shap_num_samples)\n",
    "shaps, predicted_action = shap_values.compute_shap_values()\n",
    "print(shaps)\n",
    "print(predicted_action)\n",
    "# TODO: Change SHAP parameters based on new parameters dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# Explain how to run the performance and what all the results mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# Explain how to access documentation in the browser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
