{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExplainableRL: Onboarding recipe\n",
    "In this notebook, a template sequence of steps to use the ExplainableRL library is described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: give a bit of context and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import *\n",
    "\n",
    "# Import functions\n",
    "from src.foundation.engine import Engine\n",
    "from src.data_handler.data_handler import DataHandler\n",
    "from src.explainability.pdp import PDP\n",
    "from src.explainability.shap_values import ShapValues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain each parameter and when to use what\n",
    "hyperparam_dict_ds_data_suggest = {\n",
    "    \"dimensions\": {'states': {'lead_time': 10,\n",
    "                                'length_of_stay': 10,\n",
    "                                'competitor_price_difference_bin': 4,\n",
    "                                'demand_bin': 4,\n",
    "                                'price': 4},\n",
    "                    'actions': {'price': 10},\n",
    "                    'rewards': ['reward']\n",
    "                    },\n",
    "\n",
    "    \"dataset\": {'data_path': 'data/ds-data/my_example_data.parquet',\n",
    "                'col_delimiter': '|',\n",
    "                'n_samples': 1000,\n",
    "                'normalisation': True},\n",
    "\n",
    "    \"training\": {'env_type': 'strategic_pricing_suggest',\n",
    "                    'num_episodes': 500,\n",
    "                    'num_steps': 1,\n",
    "                    'train_test_split': 0.2,\n",
    "                    'evaluate': False,\n",
    "                    'num_eval_steps': 1},\n",
    "\n",
    "    \"agent\": {'agent_type': 'q_learner',\n",
    "                \"gamma\": 0.3,\n",
    "                \"epsilon\": 0.4,\n",
    "                \"epsilon_decay\": 0.1,\n",
    "                \"epsilon_minimum\": 0.1,\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"learning_rate_decay\": 0.1,\n",
    "                \"learning_rate_minimum\": 0.1,\n",
    "                \"lambda\": 0.2,\n",
    "                \"use_uncertainty\": False,\n",
    "                \"q_importance\": 0.7,\n",
    "                },\n",
    "\n",
    "    \"explainability\": {'shap_num_samples': 1},\n",
    "\n",
    "    \"program_flow\": {\"verbose\": False}\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: overview of what to initialise (DataHandler & Engine with Agent and Env inside)\n",
    "# TODO: If needed by evaluation/explainability/performance add a new dictionary and explain it\n",
    "hyperparam_dict = hyperparam_dict_ds_data_suggest\n",
    "verbose = hyperparam_dict['program_flow']['verbose']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a. Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to define the functions to load data from the path, and split it into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain data strucuture needed for both Env & how DataHanlder is built\n",
    "\n",
    "# TODO: Load and split the data in train and test and save the test set\n",
    "def load_data(data_path, delimiter=','):\n",
    "    \"\"\"Load data from file.\n",
    "\n",
    "    Args:\n",
    "        delimiter (str): Which separates columns.\n",
    "    \"\"\"\n",
    "    file_type = data_path.split('.')[-1]\n",
    "    if file_type == 'csv':\n",
    "        dataset = pd.read_csv(data_path, sep=delimiter)\n",
    "    elif file_type == 'xlsx':\n",
    "        dataset = pd.read_excel(data_path)\n",
    "    elif file_type == 'parquet':\n",
    "        dataset = pd.read_parquet(data_path)\n",
    "    else:\n",
    "        raise ValueError(\"File type not supported\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_train_test(dataset, train_test_split=0.2):\n",
    "    \"\"\"Split dataset into train and test.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Dataset.\n",
    "        train_test_split (float): Proportion of test data.\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset (pd.DataFrame): Train dataset.\n",
    "        test_dataset (pd.DataFrame): Test dataset.\n",
    "    \"\"\"\n",
    "    msk = np.random.rand(len(dataset)) < train_test_split\n",
    "    return dataset[msk], dataset[~msk]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and split it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(hyperparam_dict['dataset']['data_path'])\n",
    "train_dataset, test_dataset = split_train_test(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Datahandler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Load data\")\n",
    "dh = DataHandler(hyperparam_dict=hyperparam_dict, dataset=train_dataset, test_dataset=test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.prepare_data_for_engine(col_delimiter=hyperparam_dict['dataset']['col_delimiter'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b. Engine Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Initialise Engine\")\n",
    "engine = Engine(dh=dh, hyperparam_dict=hyperparam_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c. Environment and Agent Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Create the world\")\n",
    "engine.create_world()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Agent Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    n_samples = hyperparam_dict['n_samples']\n",
    "    n_episodes = hyperparam_dict['n_episodes']\n",
    "    print(f\"{timestamp}: Train the agent on {n_samples} samples and {n_episodes} episodes\")\n",
    "engine.train_agent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b. Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# TODO: Add saving and uploading process in pickle\n",
    "# TODO: Take path and file name from dictionary\n",
    "import pickle\n",
    "\n",
    "with open('runs/engine.pkl','wb') as f:\n",
    "    pickle.dump(engine, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Evaluator class. The Evaluator class provides custom Reinforcement Learning Plots (such as cumulative reward per episode, or average reward distribution) averaged across different runs. It also provides other metrics such as: total agent reward versus historical reward on the test set, or action accuracy versus historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Evaluator.__init__() got an unexpected keyword argument 'engine_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/usuari/Desktop/Imperial/mscai/Term 2/software-engineering-group/explainable-RL/onboarding_nb.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/usuari/Desktop/Imperial/mscai/Term%202/software-engineering-group/explainable-RL/onboarding_nb.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m Evaluator\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/usuari/Desktop/Imperial/mscai/Term%202/software-engineering-group/explainable-RL/onboarding_nb.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m evaluator \u001b[39m=\u001b[39m Evaluator(engine_path \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mruns/engine.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Evaluator.__init__() got an unexpected keyword argument 'engine_path'"
     ]
    }
   ],
   "source": [
    "from src.evaluation.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(engine_path = 'runs/engine.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative reward versus training episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot average reward distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation on explainability framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a. PDP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Show PDPs plots\")\n",
    "pdp = PDP(engine=engine)\n",
    "pdp.build_data_for_plots(engine.agent.Q, engine.agent.Q_num_samples)\n",
    "type_features = hyperparam_dict['feature_types']\n",
    "fig_name = \"PDP plots - All states\"\n",
    "pdp.plot_pdp(states_names=state_labels, \n",
    "             fig_name=fig_name,\n",
    "             type_features=type_features, \n",
    "             savefig=True, \n",
    "             all_states=True)\n",
    "# TODO: Change PDP parameters based on new parameters dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b. Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "if verbose:\n",
    "    timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(f\"{timestamp}: Show SHAP values plots\")\n",
    "shap_values = ShapValues(sample=[8, 1, 1, 1],\n",
    "                         engine=engine,\n",
    "                         number_of_samples=shap_num_samples)\n",
    "shaps, predicted_action = shap_values.compute_shap_values()\n",
    "print(shaps)\n",
    "print(predicted_action)\n",
    "# TODO: Change SHAP parameters based on new parameters dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# Explain how to run the performance and what all the results mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add explanation\n",
    "# Explain how to access documentation in the browser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
