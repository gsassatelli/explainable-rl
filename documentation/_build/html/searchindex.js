Search.setIndex({"docnames": ["data_handler", "foundation", "index", "modules"], "filenames": ["data_handler.rst", "foundation.rst", "index.rst", "modules.rst"], "titles": ["data-handler package", "foundation package", "Welcome to Explainable Reinforcement Learning Documentation\u2019s documentation!", "explainable-RL"], "terms": {"class": [0, 1], "state": [0, 1], "q_valu": [], "polici": 1, "env": 1, "base": [0, 1], "object": [0, 1], "create_ag": 1, "reset": 1, "call": 1, "when": 1, "episod": 1, "start": 1, "epsilon_greedy_polici": 1, "return": [0, 1], "epsilon": 1, "greedi": 1, "action": [0, 1], "fit": 1, "n_episod": 1, "n_step": 1, "dataset": [0, 1], "todo": [0, 1], "decai": 1, "update_q_valu": 1, "next_stat": 1, "reward": [0, 1], "updat": 1, "q": 1, "tabl": 1, "us": 1, "bellman": 1, "rl": 2, "data": [2, 3], "handler": [2, 3], "packag": [2, 3], "foundat": [2, 3], "index": 2, "modul": [2, 3], "search": 2, "page": 2, "submodul": 3, "data_handl": 3, "content": 3, "agent": 3, "engin": [0, 3], "environ": 3, "gamma": 1, "float": 1, "0": 1, "9": 1, "paramet": [0, 1], "current": 1, "implement": 1, "part": 1, "int": 1, "lr": 1, "1": 1, "number": 1, "train": [0, 1], "maximum": 1, "step": 1, "within": 1, "each": 1, "learn": 1, "rate": 1, "initialize_ag": 1, "initi": 1, "list": [0, 1], "equat": 1, "select": [0, 1], "which": [0, 1], "transit": 1, "obtain": 1, "relat": 1, "necessari": 1, "now": 1, "becaus": 1, "we": 1, "have": 1, "myopic": 1, "mdp_data": 1, "datafram": [0, 1], "agent_typ": 1, "str": [0, 1], "env_typ": 1, "num_episod": 1, "num_step": 1, "creat": 1, "an": 1, "store": [0, 1], "create_env": 1, "create_world": 1, "mdp": 1, "instanc": 1, "given": 1, "task": 1, "episode_flag": 1, "evalu": 1, "particular": 1, "need": [0, 1], "predict": 1, "type": 1, "action_reward": 1, "ensur": 1, "here": 1, "output": 1, "max": 1, "valu": 1, "NO": 1, "explor": 1, "get_result": 1, "get": [0, 1], "result": 1, "next": 1, "sprint": 1, "compar": 1, "2": 1, "thi": 1, "could": 1, "averag": 1, "after": 1, "converg": 1, "q_tabl": 1, "save_paramet": 1, "save": 1, "dure": 1, "e": 1, "g": 1, "ani": 1, "other": 1, "Not": 1, "sure": 1, "function": 1, "can": 1, "directli": 1, "alreadi": 1, "contain": 1, "remov": 1, "train_ag": 1, "chosen": 1, "discount_factor": 1, "defin": 1, "instanti": 1, "bin_state_action_spac": 1, "zip": 1, "bin": 1, "pair": 1, "group": 1, "per": 1, "datapoint": 1, "np": 1, "arrai": 1, "create_average_reward_matrix": 1, "bins_dict": 1, "gener": 1, "spars": 1, "matrix": 1, "dict": 1, "dictionari": 1, "count": 1, "sum": 1, "associ": 1, "coo": 1, "get_counts_and_rewards_per_bin": 1, "initialise_env": 1, "inform": 1, "join_state_act": 1, "join": 1, "togeth": 1, "make_rewards_from_data": 1, "from": 1, "input": 1, "randomis": 1, "take": 1, "done": 1, "flag": 1, "mean": [0, 1], "termin": 1, "tupl": 1, "datahandl": 0, "data_path": 0, "state_label": 0, "action_label": 0, "reward_label": 0, "preprocess": 0, "get_act": 0, "taken": 0, "pd": 0, "get_reward": 0, "get_stat": 0, "load_data": 0, "delimit": 0, "load": 0, "csv": 0, "normalise_dataset": 0, "cols_to_norm": 0, "none": 0, "normalis": 0, "centr": 0, "zero": 0, "varianc": 0, "one": 0, "column": 0, "name": 0, "prepare_data_for_engin": 0, "col_delimit": 0, "cols_to_normalis": 0, "prepar": 0, "preprocess_data": 0, "bool": 0, "true": 0, "columns_to_normalis": 0, "space": 0, "appli": 0, "shuffl": 0, "split": 0, "left": 0, "empti": 0, "all": 0, "extens": 0, "aggreg": 0, "over": 0, "time": 0, "period": 0, "reverse_norm": 0, "revers": 0}, "objects": {"": [[0, 0, 0, "-", "data_handler"], [1, 0, 0, "-", "foundation"]], "data_handler": [[0, 0, 0, "-", "data_handler"]], "data_handler.data_handler": [[0, 1, 1, "", "DataHandler"]], "data_handler.data_handler.DataHandler": [[0, 2, 1, "", "get_actions"], [0, 2, 1, "", "get_rewards"], [0, 2, 1, "", "get_states"], [0, 2, 1, "", "load_data"], [0, 2, 1, "", "normalise_dataset"], [0, 2, 1, "", "prepare_data_for_engine"], [0, 2, 1, "", "preprocess_data"], [0, 2, 1, "", "reverse_norm"]], "foundation": [[1, 0, 0, "-", "agent"], [1, 0, 0, "-", "engine"], [1, 0, 0, "-", "environment"]], "foundation.agent": [[1, 1, 1, "", "Agent"]], "foundation.agent.Agent": [[1, 2, 1, "", "epsilon_greedy_policy"], [1, 2, 1, "", "fit"], [1, 2, 1, "", "initialize_agent"], [1, 2, 1, "", "update_q_values"]], "foundation.engine": [[1, 1, 1, "", "Engine"]], "foundation.engine.Engine": [[1, 3, 1, "", "agent"], [1, 3, 1, "", "agent_type"], [1, 2, 1, "", "create_agent"], [1, 2, 1, "", "create_env"], [1, 2, 1, "", "create_world"], [1, 3, 1, "", "env"], [1, 3, 1, "", "env_type"], [1, 3, 1, "", "episode_flag"], [1, 2, 1, "", "evaluate"], [1, 3, 1, "", "gamma"], [1, 2, 1, "", "get_results"], [1, 3, 1, "", "mdp_data"], [1, 3, 1, "", "num_episodes"], [1, 3, 1, "", "num_steps"], [1, 3, 1, "", "policy"], [1, 3, 1, "", "q_table"], [1, 2, 1, "", "save_parameters"], [1, 2, 1, "", "train_agent"]], "foundation.environment": [[1, 1, 1, "", "MDP"]], "foundation.environment.MDP": [[1, 2, 1, "", "bin_state_action_space"], [1, 2, 1, "", "create_average_reward_matrix"], [1, 2, 1, "", "get_counts_and_rewards_per_bin"], [1, 2, 1, "", "initialise_env"], [1, 2, 1, "", "join_state_action"], [1, 2, 1, "", "make_rewards_from_data"], [1, 2, 1, "", "reset"], [1, 2, 1, "", "step"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"data": 0, "handler": 0, "packag": [0, 1], "submodul": [0, 1], "data_handl": 0, "modul": [0, 1], "content": [0, 1, 2], "foundat": 1, "agent": 1, "engin": 1, "environ": 1, "welcom": 2, "explain": [2, 3], "reinforc": 2, "learn": 2, "document": 2, "s": 2, "indic": 2, "tabl": 2, "rl": 3}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 56}})