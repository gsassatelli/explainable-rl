{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.foundation.engine import Engine\n",
    "from src.data_handler.data_handler import DataHandler\n",
    "from src.explainability.pdp import PDP\n",
    "from datetime import datetime\n",
    "import ipdb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper-parameters for DS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_dict_ds_data = {\n",
    "        'states': ['lead_time', 'length_of_stay',\n",
    "                   'competitor_price_difference_bin', 'demand_bin'],\n",
    "        'actions': ['price'],\n",
    "        'bins': [10, 10, 4, 4, 10], #TODO: these correspond to the states and actions. Probably should change to a dict.\n",
    "        'rewards': ['reward'],\n",
    "        'feature_types': {\n",
    "            'lead_time': \"continuous\",\n",
    "            'length_of_stay': \"continuous\",\n",
    "            'competitor_price_difference_bin': \"discrete\",\n",
    "            'demand_bin': \"discrete\",\n",
    "            'price': \"continuous\",\n",
    "            'reward': \"continuous\"\n",
    "        },\n",
    "        'n_samples': 100000,\n",
    "        'data_path': 'data/ds-data/my_example_data.parquet',\n",
    "        'col_delimiter': '|',\n",
    "        'cols_to_normalise': ['lead_time', 'length_of_stay',\n",
    "                   'competitor_price_difference_bin', 'demand_bin', 'price', 'reward'],\n",
    "        'agent_type': 'q_learner',\n",
    "        'env_type': 'strategic_pricing',\n",
    "        'num_episodes': 100000,\n",
    "        'num_steps': 1,\n",
    "        'train_test_split': 0.2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"{timestamp}: Load data\")\n",
    "states = hyperparam_dict['states']\n",
    "actions = hyperparam_dict['actions']\n",
    "rewards = hyperparam_dict['rewards']\n",
    "n_samples = hyperparam_dict['n_samples']\n",
    "dh = DataHandler(data_path=hyperparam_dict['data_path'],\n",
    "                    state_labels=states,\n",
    "                    action_labels=actions,\n",
    "                    reward_labels=rewards,\n",
    "                    n_samples=n_samples)\n",
    "\n",
    "# Preprocess the data\n",
    "timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"{timestamp}: Preprocess data\")\n",
    "dh.prepare_data_for_engine(col_delimiter=hyperparam_dict['col_delimiter'],\n",
    "                            cols_to_normalise=hyperparam_dict[\n",
    "                                'cols_to_normalise'])\n",
    "\n",
    "# Create engine\n",
    "timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(f\"{timestamp}: Initialize Engine\")\n",
    "engine = Engine(dh,\n",
    "                agent_type=hyperparam_dict['agent_type'],\n",
    "                env_type=hyperparam_dict['env_type'],\n",
    "                num_episodes=hyperparam_dict['num_episodes'],\n",
    "                num_steps=hyperparam_dict['num_steps'],\n",
    "                bins=hyperparam_dict['bins'],\n",
    "                train_test_split = hyperparam_dict['train_test_split']\n",
    "                )\n",
    "# Create world\n",
    "timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"{timestamp}: Create the world\")\n",
    "engine.create_world()\n",
    "\n",
    "# Train agent\n",
    "timestamp = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"{timestamp}: Train the agent on {n_samples} samples\")\n",
    "engine.train_agent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ###########################################################\n",
    "    ################ Evaluate agent ###########################\n",
    "    ###########################################################\n",
    "\n",
    "    # TODO: denorm states, actions and rewards (using datahandler's inverse scaling)\n",
    "\n",
    "    states, actions, rewards_hist, actions_agent, rewards_agent = \\\n",
    "        engine.evaluate_agent()\n",
    "    \n",
    "    # Sum obtained reward optimal vs historical policy\n",
    "    import numpy as np\n",
    "    print(f\"Return based on historical data: {np.sum(rewards_hist)}\")\n",
    "    print(f\"Return based on agent policy: {np.sum(rewards_agent)}\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(actions, actions_agent)\n",
    "    plt.savefig('policy.png')\n",
    "\n",
    "    ipdb.set_trace()\n",
    "\n",
    "    ###########################################################\n",
    "    ################# End of evaluation #######################\n",
    "    ###########################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
